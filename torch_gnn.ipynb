{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import scipy as sp\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(name='G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    G.add_node(i, name=i)\n",
    "edges = [(1,2), (1,3), (2,4), (2,5), (3,4), (3,6) ]\n",
    "G.add_edges_from(edges)\n",
    "nx.draw_networkx(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48894/1151363033.py:1: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
      "/tmp/ipykernel_48894/1151363033.py:2: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n"
     ]
    }
   ],
   "source": [
    "A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
    "X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n",
    "X = np.expand_dims(X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AX = np.dot(A,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_self_loops = G.copy()\n",
    "\n",
    "self_loops = []\n",
    "for i in range(1, 1+ G.number_of_nodes()):\n",
    "    self_loops.append((i,i))\n",
    "\n",
    "G_self_loops.add_edges_from(self_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges of G with self-loops:\n",
      " [(1, 2), (1, 3), (1, 1), (2, 4), (2, 5), (2, 2), (3, 4), (3, 6), (3, 3), (4, 4), (5, 5), (6, 6)]\n"
     ]
    }
   ],
   "source": [
    "print('Edges of G with self-loops:\\n', G_self_loops.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48894/2223732105.py:1: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])\n"
     ]
    }
   ],
   "source": [
    "A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_hatX:\n",
      " [[ 6.]\n",
      " [12.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]]\n"
     ]
    }
   ],
   "source": [
    "A_hatX = np.dot(A_hat, X)\n",
    "print('A_hatX:\\n', A_hatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3], [2, 4], [3, 4], [4, 3], [5, 2], [6, 2]]\n"
     ]
    }
   ],
   "source": [
    "edge_List = G_self_loops.edges() \n",
    "Deg_Mat = [[i, 0] for i in G_self_loops.nodes()]\n",
    "\n",
    "for element in edge_List:\n",
    "  if element[0] != element[1]:\n",
    "    Deg_Mat[element[0] - 1][1] += 1\n",
    "    Deg_Mat[element[1] - 1][1] += 1\n",
    "  else :\n",
    "    Deg_Mat[element[0] - 1][1] += 1\n",
    "\n",
    "print(Deg_Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Matrix of added self-loops G as numpy array (D):\n",
      " [[3 0 0 0 0 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 0 4 0 0 0]\n",
      " [0 0 0 3 0 0]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "D = np.diag([deg for [n,deg] in Deg_Mat]) \n",
    "print('Degree Matrix of added self-loops G as numpy array (D):\\n', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of D:\n",
      " [[0.33333333 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.         0.         0.         0.        ]\n",
      " [0.         0.         0.25       0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "D_inv = np.linalg.inv(D)\n",
    "print('Inverse of D:\\n', D_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333 0.         0.         0.        ]\n",
      " [0.25       0.25       0.         0.25       0.25       0.        ]\n",
      " [0.25       0.         0.25       0.25       0.         0.25      ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.5        0.         0.         0.5        0.        ]\n",
      " [0.         0.         0.5        0.         0.         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "D_invA = np.dot(D_inv, A_hat)\n",
    "print(D_invA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAXW:\n",
      " [[2. ]\n",
      " [3. ]\n",
      " [3.5]\n",
      " [3. ]\n",
      " [3.5]\n",
      " [4.5]]\n"
     ]
    }
   ],
   "source": [
    "DAX = np.dot(D_invA,X)\n",
    "\n",
    "print('DAXW:\\n', DAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-27 15:24:14--  https://www.dropbox.com/s/fl9mvrio3hah4on/cora.content\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/fl9mvrio3hah4on/cora.content [following]\n",
      "--2023-04-27 15:24:15--  https://www.dropbox.com/s/raw/fl9mvrio3hah4on/cora.content\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com/cd/0/inline/B68XqoWPW6kn_4otGfAn0PabH87XsRY0isetQHUteITEPYYscojpV0KMZnZD7bSHsLIvj74AJD4APGBIbQsUtjIt-Q36Ghzx9QBM8gwQBd8PsEYE1EPKbsYZcoGGGs6vhP6WgSFQj2fz6NxPQatJlEh8b78reB4pp0vWGL-rSWzvXQ/file# [following]\n",
      "--2023-04-27 15:24:15--  https://uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com/cd/0/inline/B68XqoWPW6kn_4otGfAn0PabH87XsRY0isetQHUteITEPYYscojpV0KMZnZD7bSHsLIvj74AJD4APGBIbQsUtjIt-Q36Ghzx9QBM8gwQBd8PsEYE1EPKbsYZcoGGGs6vhP6WgSFQj2fz6NxPQatJlEh8b78reB4pp0vWGL-rSWzvXQ/file\n",
      "Resolving uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com (uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6031:15::a27d:510f\n",
      "Connecting to uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com (uc569247362c1e362babfb4d10bc.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7823427 (7.5M) [text/plain]\n",
      "Saving to: ‘cora.content.3’\n",
      "\n",
      "cora.content.3      100%[===================>]   7.46M  2.65MB/s    in 2.8s    \n",
      "\n",
      "2023-04-27 15:24:20 (2.65 MB/s) - ‘cora.content.3’ saved [7823427/7823427]\n",
      "\n",
      "--2023-04-27 15:24:20--  https://www.dropbox.com/s/l829sldp7xqrt0h/cora.cites\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
      "HTTP request sent, awaiting response... ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/fl9mvrio3hah4on/cora.content\n",
    "!wget https://www.dropbox.com/s/l829sldp7xqrt0h/cora.cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>62389</td>\n",
       "      <td>1107325</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>12198</td>\n",
       "      <td>95225</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>27543</td>\n",
       "      <td>1135125</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>179702</td>\n",
       "      <td>141171</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>33895</td>\n",
       "      <td>33904</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target   source  label\n",
       "3434   62389  1107325  cites\n",
       "1816   12198    95225  cites\n",
       "2539   27543  1135125  cites\n",
       "4408  179702   141171  cites\n",
       "2811   33895    33904  cites"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "edgelist = pd.read_csv(os.path.join(\"\", \"cora.cites\"), sep='\\t', header=None, names=[\"target\", \"source\"]) # it has graph\n",
    "edgelist[\"label\"] = \"cites\"\n",
    "edgelist.sample(frac=1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1033, 35, 103482, 103515, 1050679, 1103960, 1103985, 1109199, 1112911, 1113438, 1113831, 1114331, 1117476, 1119505, 1119708, 1120431, 1123756, 1125386, 1127430, 1127913, 1128204, 1128227, 1128314, 1128453, 1128945, 1128959, 1128985, 1129018, 1129027, 1129573, 1129683, 1129778, 1130847, 1130856, 1131116, 1131360, 1131557, 1131752, 1133196, 1133338, 1136814, 1137466, 1152421, 1152508, 1153065, 1153280, 1153577, 1153853, 1153943, 1154176, 1154459, 116552, 12576, 128540, 132806, 135130, 141342, 141347, 148170, 15670, 1688, 175291, 178727, 18582, 190697, 190706, 1956, 197054, 198443, 198653, 206371, 210871, 229635, 231249, 248425, 249421, 254923, 259701, 259702, 263279, 263498, 265203, 273152, 27510, 28290, 286500, 287787, 28851, 289779, 289780, 289781, 307015, 335733, 33904, 33907, 35061, 38205, 387795, 415693, 41714, 427606, 44368, 45599, 46079, 46431, 486840, 48766, 503883, 503893, 513189, 54129, 54131, 56119, 561238, 568857, 573964, 573978, 574009, 574264, 574462, 575077, 575292, 575331, 576725, 576795, 577227, 578780, 579008, 592973, 593091, 593105, 593240, 593260, 593813, 594047, 594543, 594649, 594900, 608326, 634902, 634904, 634938, 634975, 640617, 646809, 646837, 647408, 647447, 66556, 66563, 66805, 69284, 69296, 694759, 735303, 78511, 787016, 801170, 81722, 82098, 84021, 85352, 86359, 8865, 887, 97645, 98698, 1109017, 40, 1114442, 1103315, 114, 1105394, 1106112, 1106172, 1106406, 1107455, 1111052, 1114125, 1117942, 1118245, 1118332, 1120170, 1126029, 124064, 128, 130, 136665, 191404, 193742, 23258, 28227, 28287, 28350, 28387, 28471, 28485, 341188, 38480, 39403, 434, 55968, 58540, 6155, 6170, 6196, 6220, 64484, 7432, 755217, 8213, 91975, 976334, 1109581, 117, 149669, 17476, 189708, 206259, 28202, 28278, 32872, 33013, 6214, 75674, 20526, 13960, 345340, 1118092, 288, 36167, 1135125, 424, 218666, 47684, 267003, 58454, 463, 1102364, 504, 1112650, 506, 89416, 1106546, 10796, 1105033, 1111304, 1113614, 114308, 134128, 161221, 170798, 19045, 20972, 28456, 299195, 299197, 334153, 35490, 595056, 6151, 6213, 6215, 64519, 87363, 1103979, 906, 1105344, 1114352, 1136397, 1140040, 34355, 910, 1104379, 1105530, 1108834, 1110520, 1114118, 1116569, 1118848, 1120858, 1122460, 1126044, 1129111, 1135137, 1152194, 12439, 12946, 131042, 13136, 160705, 227286, 242637, 31043, 340075, 340078, 35905, 42847, 436796, 48550, 5462, 576257, 58552, 5869, 636511, 67292, 675649, 684372, 94953, 1107010, 936, 1111899, 129558, 207395, 3084, 3828, 38845, 20180, 940, 28265, 941, 1152896, 943, 91852, 1034, 1026, 1102550, 1105231, 1129798, 1153945, 1107062, 1035, 1110515, 1154525, 1213, 409725, 8766, 102938, 1237, 1102400, 143676, 1246, 1104007, 42207, 57764, 6125, 1102625, 1272, 1108167, 1120962, 112378, 1123867, 1128256, 1129208, 1135358, 157805, 18615, 192734, 20593, 27230, 284414, 30895, 30901, 444240, 520471, 52835, 552469, 560936, 591016, 636098, 65653, 6917, 6923, 85452, 85688, 93923, 97892, 1031453, 1365, 1102407, 1105062, 1106287, 1108050, 1110494, 1110998, 1113995, 1114153, 1114388, 1114605, 1116347, 1116594, 1117653, 1119140, 1120211, 1120866, 1128839, 1129443, 1130600, 1131647, 1131745, 1131748, 1132922, 1132968, 1135368, 1136422, 1136442, 1152143, 1152821, 1154169, 120084, 139865, 157401, 171225, 184918, 188318, 188471, 22835, 23448, 23502, 23507, 237521, 26850, 330148, 340299, 39904, 49482, 562123, 628500, 648232, 649731, 69392, 7276, 7297, 77758, 782486, 83826, 85299, 853150, 90888, 93555, 948299, 948846, 949318, 949511, 950052, 950305, 1102567, 1481, 1106052, 1108267, 1111614, 1113934, 1114864, 1117184, 1119295, 1120563, 1153891, 200480, 399173, 4878, 521207, 521269, 1134022, 152483, 1694, 39474, 647413, 1115291, 1717, 1116336, 1135108, 50381, 733167, 35797, 1786, 1114502, 1817, 1108389, 1919, 129042, 1949, 3101, 3095, 1951, 1107215, 1952, 1153166, 1953, 1153724, 1153728, 110163, 1955, 1110390, 101143, 1118302, 1153101, 1153150, 263486, 83449, 1959, 3097, 310530, 38839, 73327, 82664, 1102442, 1997, 1108551, 1109439, 1109542, 129897, 154982, 3233, 49811, 7032, 1999, 1123068, 1131471, 39126, 6771, 10186, 2354, 1107140, 1113852, 1130539, 154134, 40151, 74749, 1000012, 2440, 1061127, 1106388, 1107095, 1110426, 1114512, 1117786, 1120650, 1127619, 1153254, 136766, 136768, 151430, 212777, 23546, 49843, 49844, 49847, 582343, 591017, 63931, 72908, 102406, 2653, 107177, 1104055, 1116268, 1116842, 1118764, 1119751, 1152075, 167656, 197783, 28641, 321861, 350362, 4660, 59045, 65650, 1104851, 2654, 1106630, 1107861, 1110438, 1121867, 1123926, 211906, 27250, 463825, 52784, 1115375, 2658, 1130676, 1130808, 1131607, 1132948, 1135899, 1140230, 230884, 236759, 282700, 395540, 578347, 696342, 696345, 696346, 751408, 99023, 1119671, 2663, 1105574, 2665, 1113035, 1122704, 1132486, 205192, 237489, 43639, 578306, 578309, 578337, 582139, 630817, 631052, 763009, 1108169, 2695, 1120197, 2698, 342802, 2696, 1114192, 1118083, 1123239, 1133004, 256106, 469504, 5348, 99025, 1120777, 2702, 12330, 395553, 72056, 1107171, 255233, 33303, 15889, 3085, 221302, 30973, 395725, 5062, 110162, 218682, 1106236, 129045, 110164, 70441, 103529, 3112, 1152564, 470511, 77826, 77829, 1110000, 3187, 129896, 280876, 5086, 105865, 3191, 1106789, 1127530, 1131267, 137873, 162664, 308920, 310742, 3192, 423463, 561364, 642827, 3217, 167670, 238099, 86840, 1106492, 3218, 1119987, 1120169, 1152290, 1153264, 187354, 277263, 35070, 417017, 6639, 66782, 6767, 6941, 1115677, 3220, 1125992, 1128430, 1130634, 1131728, 1132706, 120039, 145215, 346243, 36620, 39124, 40922, 429805, 654177, 69397, 8832, 1103737, 3222, 1114222, 1131137, 1132157, 964248, 3223, 100197, 3229, 1105718, 1106568, 1108209, 1109392, 1112767, 1125082, 1125895, 1126037, 1128868, 1130915, 1130927, 1130931, 1132418, 1140289, 1152277, 1152673, 1154251, 118559, 133550, 16461, 189577, 25181, 25184, 27174, 27631, 31769, 33412, 35343, 35863, 35922, 390922, 396412, 444191, 447250, 52515, 62347, 641976, 654326, 7022, 72101, 82087, 919885, 92065, 96335, 104840, 3231, 1102761, 1106330, 1106370, 1107067, 1113926, 1115471, 1128536, 1153169, 180399, 20850, 259126, 3237, 328370, 49660, 6334, 63477, 63486, 66594, 68463, 8699, 8821, 3232, 20942, 521251, 1125467, 192850, 272720, 509379, 976284, 3235, 3236, 601561, 1110531, 3240, 39130, 1103610, 3243, 1110947, 1113739, 307336, 31932, 368431, 854434, 5075, 3932, 1105428, 4274, 1114664, 1119078, 105899, 4329, 1110768, 28254, 395547, 46468, 510718, 1103676, 4330, 1104449, 11093, 1112929, 1132459, 1132461, 1136393, 1152917, 118436, 119956, 120013, 151708, 168410, 32688, 37884, 390889, 428610, 684986, 69418, 753265, 949217, 1104300, 4335, 1121254, 1136310, 1140547, 116790, 239800, 32698, 62274, 62417, 755082, 95718, 1102548, 4553, 4584, 1120020, 1130567, 13885, 13917, 293285, 6184, 6210, 628815, 164, 4637, 1105887, 4649, 1127851, 135765, 135766, 411092, 449841, 8703, 429781, 5038, 102884, 4804, 1108175, 1112574, 1153262, 12182, 12210, 157761, 25805, 45189, 68505, 77515, 989397, 7867, 4983, 5055, 28026, 5064, 5069, 1121176, 1129106, 272345, 385067, 109323, 1105698, 137849, 354004, 440815, 79809, 1152714, 5194, 133563, 133566, 140569, 139547, 75695, 911198, 5454, 1128425, 1130680, 1022969, 5600, 1117833, 5959, 152226, 545647, 582511, 5966, 1107572, 12211, 27612, 385251, 6130, 1154123, 1154124, 671269, 675847, 1106547, 1107355, 1385, 6152, 1106966, 6163, 20534, 96847, 6169, 1114629, 170338, 7419, 10981, 1120731, 13686, 399339, 1153056, 81350, 1108329, 6209, 118259, 118260, 181782, 212930, 8875, 95579, 1105764, 1109957, 1111230, 1113182, 1113459, 1117760, 1122425, 1123553, 1128267, 1129096, 1129243, 13193, 153598, 17208, 195361, 218410, 23774, 241133, 293271, 353541, 6224, 628667, 628668, 672064, 1115886, 1152740, 28447, 6378, 93320, 95589, 6216, 230300, 10793, 6238, 1123991, 1130356, 84459, 6311, 235776, 1108656, 6318, 1121057, 20833, 1152307, 1152448, 1152975, 1153703, 6343, 141868, 359067, 521252, 1112665, 6344, 1106103, 6346, 1112075, 28473, 1103162, 6385, 682666, 892139, 1102646, 6539, 116084, 178209, 568045, 1130637, 6741, 1153160, 348437, 49895, 51909, 83725, 9708, 1120444, 6925, 1111240, 6775, 350319, 6782, 100961, 6784, 1116629, 60170, 6786, 714975, 1117348, 6814, 1125469, 158172, 293974, 300071, 315266, 390896, 445938, 1105531, 6818, 50336, 50337, 1123188, 6898, 124224, 12631, 431206, 6910, 78994, 1117219, 1132083, 1152150, 1154103, 169279, 263553, 662416, 1105011, 6913, 1131230, 703953, 1153811, 1155073, 20857, 308003, 1114526, 1118658, 372862, 57922, 57948, 6935, 101660, 1115670, 1116922, 6939, 323128, 52847, 1120880, 7041, 1135082, 7047, 14549, 54844, 1136446, 7272, 763181, 7296, 763010, 1107312, 1153287, 7430, 95586, 1121603, 7532, 1153097, 141171, 314459, 7537, 80515, 1113534, 1131348, 409255, 1138027, 315789, 105057, 8079, 1108728, 8224, 1111788, 1128531, 1132815, 11342, 1153148, 1153866, 133615, 22431, 22563, 23738, 55403, 601567, 62389, 97377, 1120059, 8581, 75691, 8591, 137956, 167205, 709113, 1132809, 8594, 180187, 232605, 58268, 8617, 12359, 36145, 8619, 1102751, 8687, 38000, 8696, 101145, 173884, 27535, 308232, 502574, 51866, 59244, 89335, 1129570, 395075, 608292, 785678, 1104647, 8872, 1125092, 1152761, 1153860, 1120643, 8874, 1153816, 1152676, 8961, 1133390, 102879, 9513, 9515, 1102794, 9559, 252725, 1130780, 9581, 633585, 1120138, 9586, 33818, 1153003, 9716, 28674, 10169, 114189, 158614, 17798, 211875, 39131, 711598, 10174, 10177, 12197, 12198, 124734, 15429, 249858, 27606, 67415, 68495, 10183, 10430, 1114336, 1120713, 259772, 47570, 54550, 1103394, 10435, 208345, 22566, 41417, 1102850, 10531, 1107567, 1129442, 194617, 31336, 31349, 31353, 43698, 686532, 180373, 31097, 46536, 6217, 10798, 1154500, 18619, 20193, 252715, 1102873, 39199, 1129835, 11148, 1131719, 193354, 25413, 11339, 11326, 1127657, 11335, 11325, 217139, 11337, 211432, 44017, 45061, 1126350, 12155, 12199, 12158, 148399, 12165, 1112099, 1119471, 12169, 1059953, 1106418, 1117249, 1153183, 155736, 219239, 28632, 28640, 309476, 94713, 27199, 12194, 1107319, 12195, 1131611, 1132285, 38722, 51180, 111866, 95225, 12247, 101263, 12238, 107569, 1152490, 1153861, 156977, 213279, 400356, 12337, 16451, 16470, 16474, 12347, 101811, 12350, 152227, 20601, 20602, 1131634, 1071981, 1104999, 1105221, 1107674, 127033, 416455, 56112, 56709, 574710, 575795, 62718, 63832, 12638, 119761, 899119, 13024, 13195, 1105148, 13205, 13208, 131318, 13212, 214472, 358884, 411005, 13213, 13269, 13654, 13652, 13656, 83746, 1115959, 119686, 987188, 13658, 294239, 13717, 37998, 13966, 1126050, 13972, 34979, 13982, 14062, 1152358, 646836, 97390, 98693, 1103016, 14083, 14090, 643069, 1103031, 14428, 1103969, 14429, 14431, 34082, 73119, 1119216, 14430, 1103038, 14529, 239829, 14531, 1105932, 1152308, 56167, 592830, 60682, 14545, 14807, 264347, 25702, 15076, 708945, 175576, 210309, 217115, 35854, 41666, 89547, 15431, 12558, 1110024, 1118388, 15892, 175909, 1116146, 158098, 15984, 15987, 523394, 653441, 714289, 16008, 1131165, 189571, 189572, 152219, 16437, 430329, 51831, 1105603, 1129621, 1104787, 16471, 1114992, 273949, 1123087, 16476, 105856, 16485, 1109891, 1120049, 1131167, 16819, 1131236, 1131274, 1131312, 126793, 643003, 643221, 644093, 644334, 646195, 646286, 1152259, 16843, 1152991, 17201, 1126012, 184157, 95435, 1152633, 17242, 17363, 17477, 17488, 1107136, 17811, 245955, 17821, 18251, 18313, 86923, 18532, 1106854, 18536, 424540, 1153091, 28964, 18770, 531348, 531351, 18773, 88356, 73146, 18774, 103537, 18777, 1112686, 173863, 66794, 79817, 18781, 18785, 1106401, 18811, 20920, 18812, 510715, 18815, 18832, 18833, 18834, 1152944, 593210, 593328, 593329, 19231, 1153736, 12960, 30934, 686061, 19621, 1128846, 1131464, 123825, 240791, 628888, 649730, 649739, 66990, 853116, 948147, 19697, 40124, 38829, 20178, 64271, 91853, 20179, 95188, 1108597, 1114777, 1116397, 1116839, 1119180, 112813, 1130653, 1130657, 1138091, 1152244, 1153877, 1153879, 1153889, 144330, 566488, 566653, 566664, 20528, 70442, 1106849, 20584, 1118823, 20592, 389715, 1118209, 1121537, 20821, 1127863, 1115701, 20923, 1116530, 20924, 289885, 294030, 1116181, 1103383, 22229, 1107418, 1128369, 144701, 22241, 243483, 459216, 595193, 22386, 1128407, 38846, 107251, 107252, 1121459, 22564, 36140, 63915, 94229, 1110563, 22876, 22869, 22874, 22875, 22883, 1107367, 22886, 23070, 23069, 74700, 1116044, 134307, 134316, 74698, 87915, 1105433, 23116, 1111978, 152731, 217852, 101662, 1153064, 189574, 27249, 84020, 1111265, 24043, 928873, 1103499, 24476, 1153024, 24530, 24966, 1106671, 1123576, 1131149, 1154042, 124828, 145134, 145176, 197452, 202639, 27627, 63549, 65212, 671293, 95719, 1104258, 24974, 1112723, 34315, 40125, 285675, 385572, 1153897, 147870, 1104769, 25772, 1122580, 1126503, 641956, 25791, 45212, 25794, 1110028, 1121063, 248119, 27203, 23545, 27241, 27243, 27246, 1128990, 27514, 27530, 1128542, 27531, 592826, 1112026, 34961, 27543, 27623, 1104182, 27632, 686015, 686030, 116021, 27895, 325497, 28230, 28249, 1152436, 28267, 308529, 567005, 108047, 28336, 28359, 28385, 118558, 28389, 28412, 194645, 28487, 28489, 1152910, 28491, 1139928, 28504, 131315, 365294, 28542, 32260, 28649, 155738, 578669, 595157, 28957, 159897, 1125402, 1125944, 1112426, 29492, 1122574, 131117, 144408, 29708, 400473, 29723, 155277, 29738, 30817, 144679, 31055, 31083, 48066, 1105672, 31105, 1063773, 1124844, 1129608, 1135746, 1152162, 1152272, 1152904, 286562, 31927, 686559, 31479, 39165, 31483, 118682, 31489, 40583, 40605, 632796, 632874, 632935, 633721, 67245, 67246, 358894, 31863, 91581, 1129572, 32083, 1153933, 200630, 346292, 45605, 688361, 32276, 174418, 636500, 84695, 33231, 1132887, 33301, 1110256, 33325, 124296, 34708, 78549, 78552, 78557, 1119004, 33823, 33895, 1110546, 1106771, 34257, 1111186, 1114398, 1115456, 1116974, 1122642, 192870, 34263, 34266, 368605, 87482, 90655, 503871, 682815, 168958, 35335, 59772, 1104031, 1116410, 108962, 35778, 108983, 519318, 1128974, 399370, 60169, 627024, 35852, 41732, 134060, 1105116, 194223, 390894, 66982, 36131, 77438, 350373, 46500, 36162, 189620, 36802, 189856, 37888, 589923, 590022, 37483, 37541, 260979, 37879, 1128997, 1129610, 1153942, 117316, 592975, 592986, 593060, 606647, 61069, 1131270, 38537, 1131277, 137868, 153063, 642847, 1104191, 38771, 1110579, 39127, 1128881, 1128927, 1128935, 116081, 116087, 195150, 46476, 75972, 39210, 66986, 1123689, 39890, 1154229, 1154232, 1154233, 242663, 51045, 521855, 559804, 714256, 118079, 40131, 40135, 1104261, 884094, 40886, 41216, 1113551, 128383, 943087, 144212, 182094, 44455, 42156, 1118120, 1152179, 42209, 42221, 42848, 1116835, 1131195, 43165, 1135894, 43186, 206524, 48075, 1104435, 44121, 227178, 44514, 253971, 606479, 1131266, 45052, 1130929, 45188, 1105450, 1132385, 1104495, 45533, 45603, 1131639, 975567, 1152569, 46452, 46470, 46491, 137380, 46547, 51834, 1153275, 46501, 46887, 1153106, 1125393, 47682, 47683, 1124837, 47839, 48555, 48764, 48768, 48781, 1134865, 397488, 423816, 1112369, 217984, 49753, 49720, 683355, 123556, 50354, 289088, 1104749, 50807, 50838, 73972, 50980, 1134320, 51049, 51052, 51879, 51934, 1104809, 52000, 52007, 52003, 1112194, 300806, 446271, 53942, 1128291, 1128319, 1136791, 117315, 578645, 578646, 578649, 593022, 1128982, 141324, 459213, 459214, 593859, 593942, 59715, 54132, 593921, 594387, 62634, 1104946, 55770, 55801, 56115, 1119178, 239810, 56708, 57119, 1132731, 711527, 1154524, 57773, 235670, 57932, 1114239, 58436, 87417, 58453, 1113828, 248395, 58758, 1128208, 576973, 59626, 96845, 59798, 60159, 61073, 61312, 1106764, 61417, 94416, 1131565, 62329, 195792, 251756, 593155, 650834, 62333, 1107325, 1107558, 294145, 62607, 62676, 312409, 83461, 1105360, 63812, 63835, 64319, 1125909, 65057, 519353, 1110209, 65074, 1117920, 142268, 714748, 1112319, 1123530, 66564, 362926, 66596, 1135750, 66751, 1138043, 573535, 693143, 695284, 1105505, 1153031, 509315, 82090, 66809, 1117501, 171954, 1127558, 67584, 1127566, 562067, 67633, 68115, 68224, 231198, 69198, 70281, 70444, 70520, 70970, 593068, 71336, 71736, 71904, 1135122, 1118347, 72406, 899085, 72805, 954315, 73162, 1134348, 189774, 714260, 714879, 1105810, 73323, 73712, 74427, 1120252, 74821, 1131150, 1105877, 74920, 74921, 74937, 1107041, 74975, 75121, 103430, 75318, 1121569, 75693, 75694, 75969, 1128856, 75983, 1115790, 77108, 77112, 106590, 1129994, 1129907, 613409, 78508, 575402, 78555, 662279, 662572, 1121659, 1131466, 358866, 80491, 80656, 81714, 82666, 82920, 1125492, 1128198, 1129367, 576691, 83847, 1130678, 1106298, 1133047, 509233, 85324, 628751, 577086, 85449, 86258, 1133469, 1114184, 1153786, 89308, 103528, 137130, 1116328, 1152379, 237376, 90470, 91038, 92589, 93273, 93318, 1152958, 143801, 284023, 284025, 93755, 1111733, 94639, 94641, 116553, 95198, 95588, 95594, 95597, 95642, 990075, 594025, 96851, 100935, 99030, 193931, 100701, 596075, 101261, 102061, 102939, 1154074, 1113742, 246618, 103531, 656048, 126912, 103543, 126927, 289085, 193932, 1095507, 1107385, 1153899, 578898, 108963, 310653, 108974, 1133417, 683404, 110041, 111676, 1119623, 111770, 112099, 112787, 1125258, 114966, 1107728, 115188, 1107808, 116512, 116528, 116545, 1115166, 117328, 118424, 118435, 1121739, 1109566, 118873, 119712, 1140543, 1109873, 120817, 1152394, 121792, 141160, 1154276, 1119742, 124952, 1126011, 189721, 1108258, 126867, 126868, 126909, 126920, 645897, 126926, 1114364, 127940, 243274, 128202, 128203, 1108363, 1121313, 907845, 129287, 131122, 131317, 132821, 133553, 133567, 1108570, 133628, 481073, 134199, 164885, 447224, 134219, 134314, 134315, 134320, 135464, 1135589, 135798, 136767, 137359, 1118286, 137790, 1131300, 1154068, 139738, 1108841, 140005, 1131345, 917493, 503877, 608191, 141596, 143323, 143476, 1131549, 219446, 1128975, 1136342, 595063, 294126, 145315, 649944, 145384, 1109185, 1132434, 1109208, 148341, 1123215, 149139, 1120786, 987197, 583318, 1131223, 561568, 561581, 561593, 561595, 561610, 561613, 645084, 1112106, 154023, 1133846, 154047, 1130934, 1133028, 397590, 155158, 156794, 1118017, 1127812, 158812, 1152711, 159084, 159085, 241821, 1128853, 160732, 1153922, 1109830, 162075, 162080, 737204, 1135345, 739707, 163235, 166420, 166825, 166989, 168332, 169280, 656231, 1138755, 820662, 174425, 175256, 175548, 689152, 753070, 177115, 177993, 177998, 178718, 179180, 179702, 1152859, 179706, 1110628, 180301, 1120084, 182093, 1131550, 650814, 1110950, 187260, 1153014, 189566, 189623, 189655, 1129518, 1134346, 190698, 191216, 191222, 193347, 612306, 193352, 193918, 194609, 1154520, 126128, 215912, 563613, 248823, 377303, 198866, 199571, 643734, 202520, 202522, 643597, 203646, 205196, 1130568, 1130586, 628764, 628766, 815096, 950986, 1128151, 1129629, 210872, 1112071, 212097, 212107, 213246, 1117618, 216877, 216878, 1152663, 1112417, 219218, 567018, 219976, 220420, 226698, 1122304, 1128946, 228992, 228990, 230879, 1133428, 696343, 851968, 1153195, 232606, 1113084, 232860, 12275, 233106, 630890, 235678, 235679, 689439, 235683, 238401, 240321, 1113541, 245288, 1121398, 248431, 250566, 253762, 255628, 258259, 260121, 261040, 262108, 262121, 262178, 1138968, 263069, 263482, 264556, 335042, 267824, 270456, 1114838, 270600, 278394, 278403, 1152959, 285687, 286513, 288107, 1153784, 289945, 578845, 292277, 1133010, 1152858, 302545, 1131198, 643199, 307656, 270085, 1139195, 318071, 318187, 1117049, 321004, 1117089, 325314, 330208, 337766, 348305, 1118546, 358887, 360028, 1119211, 367312, 746058, 400455, 368657, 370366, 375605, 375825, 1119654, 376704, 379288, 60560, 380341, 1120019, 384428, 390693, 684972, 1135115, 1154173, 408885, 416867, 416964, 421481, 1105622, 430574, 1132416, 430711, 671052, 645571, 446610, 1123093, 1123493, 458439, 459206, 466170, 1128977, 467383, 1131374, 1154076, 577331, 646913, 1125597, 1125906, 1125953, 521183, 1125993, 1132864, 522338, 683360, 523010, 523574, 1126315, 529165, 561582, 646440, 561611, 1127541, 561674, 1127551, 561789, 561809, 562940, 1127810, 573553, 1138970, 593544, 608190, 1131734, 576362, 1128201, 578365, 1153900, 578650, 1128943, 1128978, 593559, 593560, 579108, 1128437, 582349, 592993, 592996, 593248, 1129015, 593104, 593201, 593209, 1153896, 594011, 594039, 1129021, 594119, 1129040, 594483, 594511, 601462, 1129368, 1129369, 1129494, 604073, 1153946, 610529, 616336, 1130069, 617378, 1130080, 617575, 1130243, 621555, 1130454, 626530, 1154012, 626531, 626574, 626999, 628459, 628458, 631015, 633031, 633030, 633081, 672070, 672071, 1131163, 642593, 1131164, 642621, 1131258, 642641, 642681, 644441, 1131172, 642798, 644470, 1131180, 642894, 1131301, 1131335, 643239, 643485, 645046, 645870, 646334, 646357, 1131184, 642920, 642930, 1131189, 1131192, 1131257, 1131305, 1131334, 644448, 644577, 643695, 643777, 643735, 645016, 644361, 644363, 644427, 1131314, 644494, 644843, 1154071, 645088, 645452, 646289, 1131330, 646412, 1131359, 646900, 647315, 1131421, 648106, 648112, 1131420, 648121, 1131414, 648369, 650807, 1133930, 653628, 1131741, 654339, 1131754, 654519, 1131828, 1132073, 662250, 1132406, 1132443, 1132505, 675756, 1132857, 682508, 683294, 684531, 687401, 1133008, 688824, 688849, 1134031, 1134056, 709518, 1134197, 711994, 714208, 1154230, 733534, 733576, 734406, 735311, 1135455, 738941, 739280, 739816, 1140548, 1135955, 752684, 1136631, 753047, 1136634, 753264, 767763, 1136040, 754594, 1136110, 756061, 1136447, 762980, 1136449, 1137140, 779960, 1138619, 814836, 815073, 820661, 817774, 1139009, 824245, 1140231, 853114, 853155, 853115, 853118]\n"
     ]
    }
   ],
   "source": [
    "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(Gnx, \"paper\", \"label\")\n",
    "\n",
    "print(Gnx.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'paper'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gnx.nodes[12210] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_1424</th>\n",
       "      <th>word_1425</th>\n",
       "      <th>word_1426</th>\n",
       "      <th>word_1427</th>\n",
       "      <th>word_1428</th>\n",
       "      <th>word_1429</th>\n",
       "      <th>word_1430</th>\n",
       "      <th>word_1431</th>\n",
       "      <th>word_1432</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37879</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_0  word_1  word_2  word_3  word_4  word_5  word_6  word_7  \\\n",
       "31336         0       0       0       0       0       0       0       0   \n",
       "1061127       0       0       0       0       0       0       0       0   \n",
       "1106406       0       0       0       0       0       0       0       0   \n",
       "13195         0       0       0       0       0       0       0       0   \n",
       "37879         0       0       0       0       0       0       0       0   \n",
       "\n",
       "         word_8  word_9  ...  word_1424  word_1425  word_1426  word_1427  \\\n",
       "31336         0       0  ...          0          0          1          0   \n",
       "1061127       0       0  ...          0          1          0          0   \n",
       "1106406       0       0  ...          0          0          0          0   \n",
       "13195         0       0  ...          0          0          0          0   \n",
       "37879         0       0  ...          0          0          0          0   \n",
       "\n",
       "         word_1428  word_1429  word_1430  word_1431  word_1432  \\\n",
       "31336            0          0          0          0          0   \n",
       "1061127          0          0          0          0          0   \n",
       "1106406          0          0          0          0          0   \n",
       "13195            0          0          0          0          0   \n",
       "37879            0          0          0          0          0   \n",
       "\n",
       "                        subject  \n",
       "31336           Neural_Networks  \n",
       "1061127           Rule_Learning  \n",
       "1106406  Reinforcement_Learning  \n",
       "13195    Reinforcement_Learning  \n",
       "37879     Probabilistic_Methods  \n",
       "\n",
       "[5 rows x 1434 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [\"word_{}\".format(ii) for ii in range(1433)]\n",
    "column_names =  feature_names + [\"subject\"]\n",
    "node_data = pd.read_csv(os.path.join(\"\", \"cora.content\"), sep='\\t', header=None, names=column_names)\n",
    "node_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case_Based',\n",
       " 'Genetic_Algorithms',\n",
       " 'Neural_Networks',\n",
       " 'Probabilistic_Methods',\n",
       " 'Reinforcement_Learning',\n",
       " 'Rule_Learning',\n",
       " 'Theory'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(node_data[\"subject\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 200\n",
    "SEED = 42\n",
    "NUM_HIDDEN = 16\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels): # we will make all class(subject) to one-hot vector for training.\n",
    "    classes = set(labels) # {'Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory'}\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx): # This part is similar to the normalization process implemented earlier.\n",
    "    #ipdb.set_trace()\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx): # Convert a scipy sparse matrix to a torch sparse tensor.\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\", dataset=\"cora\"):\n",
    "    # In the function, by using above 3 function, \n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str)) # load all tables\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # Compress sparse matrix\n",
    "    labels = encode_onehot(idx_features_labels[:, -1]) # Label onehot encoding\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # node list\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # split all nodes to train/valid/test for node classification\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \n",
    "    #Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        # initialize weight by using reset_parameters() function\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight) # XW\n",
    "        output = torch.mm(adj, support) # AXW\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "      # Obtain Node embedding\n",
    "      #ipdb.set_trace()\n",
    "      x = self.gc1(x, adj)\n",
    "      x = F.relu(x)\n",
    "      x = F.dropout(x, self.dropout, training=self.training) # For overfitting\n",
    "      x = self.gc2(x, adj)\n",
    "      x = F.log_softmax(x, dim=1) # log(softmax(x))\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "CPU times: user 4.25 s, sys: 3.22 s, total: 7.46 s\n",
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data() # adj -> adjacency matrix, same ax A,   features -> node feature matrix, same as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1], # [2708, 1433] -> [1433] for matrix multiplication of X and W\n",
    "            nhid=NUM_HIDDEN,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (0.11.4)\n",
      "Requirement already satisfied: packaging in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torchmetrics) (1.24.2)\n",
      "Requirement already satisfied: filelock in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: sympy in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/udit/anaconda3/envs/geometric/lib/python3.9/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train], task = 'multiclass', num_classes = 7)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val], task = 'multiclass', num_classes = 7)\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "def visualize(h, label, idx):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Dimension 0')\n",
    "    plt.ylabel('Dimension 1')\n",
    "\n",
    "    h_ = h[idx]\n",
    "    color = [ label[i] for i in idx ]\n",
    "    print(f'Embedding shape: {list(h_.shape)}')\n",
    "    z = TSNE(n_components=2).fit_transform(h_.detach().cpu().numpy())  \n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "def test(): # get loss and accuracy with node embedding visualization\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    visualize(output, labels, idx_test)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:00<00:02, 76.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9448 acc_train: 0.2214 loss_val: 1.9391 acc_val: 0.3367\n",
      "Epoch: 0002 loss_train: 1.9378 acc_train: 0.3071 loss_val: 1.9351 acc_val: 0.3667\n",
      "Epoch: 0003 loss_train: 1.9336 acc_train: 0.3143 loss_val: 1.9306 acc_val: 0.3667\n",
      "Epoch: 0004 loss_train: 1.9292 acc_train: 0.3714 loss_val: 1.9253 acc_val: 0.3767\n",
      "Epoch: 0005 loss_train: 1.9210 acc_train: 0.3643 loss_val: 1.9191 acc_val: 0.3833\n",
      "Epoch: 0006 loss_train: 1.9140 acc_train: 0.3857 loss_val: 1.9119 acc_val: 0.3800\n",
      "Epoch: 0007 loss_train: 1.9046 acc_train: 0.3857 loss_val: 1.9038 acc_val: 0.3800\n",
      "Epoch: 0008 loss_train: 1.8986 acc_train: 0.3714 loss_val: 1.8949 acc_val: 0.3833\n",
      "Epoch: 0009 loss_train: 1.8877 acc_train: 0.3714 loss_val: 1.8852 acc_val: 0.3800\n",
      "Epoch: 0010 loss_train: 1.8705 acc_train: 0.3929 loss_val: 1.8745 acc_val: 0.3800\n",
      "Epoch: 0011 loss_train: 1.8652 acc_train: 0.3857 loss_val: 1.8632 acc_val: 0.3800\n",
      "Epoch: 0012 loss_train: 1.8436 acc_train: 0.4000 loss_val: 1.8510 acc_val: 0.3767\n",
      "Epoch: 0013 loss_train: 1.8299 acc_train: 0.4000 loss_val: 1.8381 acc_val: 0.3700\n",
      "Epoch: 0014 loss_train: 1.8243 acc_train: 0.3571 loss_val: 1.8245 acc_val: 0.3700\n",
      "Epoch: 0015 loss_train: 1.8042 acc_train: 0.3786 loss_val: 1.8103 acc_val: 0.3667\n",
      "Epoch: 0016 loss_train: 1.7771 acc_train: 0.3857 loss_val: 1.7953 acc_val: 0.3667\n",
      "Epoch: 0017 loss_train: 1.7600 acc_train: 0.3714 loss_val: 1.7797 acc_val: 0.3633\n",
      "Epoch: 0018 loss_train: 1.7551 acc_train: 0.3714 loss_val: 1.7636 acc_val: 0.3633\n",
      "Epoch: 0019 loss_train: 1.7285 acc_train: 0.3929 loss_val: 1.7471 acc_val: 0.3633\n",
      "Epoch: 0020 loss_train: 1.7118 acc_train: 0.3643 loss_val: 1.7301 acc_val: 0.3633\n",
      "Epoch: 0021 loss_train: 1.6839 acc_train: 0.3857 loss_val: 1.7128 acc_val: 0.3633\n",
      "Epoch: 0022 loss_train: 1.6700 acc_train: 0.3929 loss_val: 1.6952 acc_val: 0.3633\n",
      "Epoch: 0023 loss_train: 1.6523 acc_train: 0.3643 loss_val: 1.6774 acc_val: 0.3633\n",
      "Epoch: 0024 loss_train: 1.6219 acc_train: 0.4000 loss_val: 1.6594 acc_val: 0.3633\n",
      "Epoch: 0025 loss_train: 1.6028 acc_train: 0.3643 loss_val: 1.6413 acc_val: 0.3633\n",
      "Epoch: 0026 loss_train: 1.5928 acc_train: 0.3786 loss_val: 1.6233 acc_val: 0.3633\n",
      "Epoch: 0027 loss_train: 1.5693 acc_train: 0.3857 loss_val: 1.6053 acc_val: 0.3633\n",
      "Epoch: 0028 loss_train: 1.5384 acc_train: 0.3929 loss_val: 1.5874 acc_val: 0.3633\n",
      "Epoch: 0029 loss_train: 1.5171 acc_train: 0.4071 loss_val: 1.5696 acc_val: 0.3667\n",
      "Epoch: 0030 loss_train: 1.5046 acc_train: 0.3714 loss_val: 1.5521 acc_val: 0.3733\n",
      "Epoch: 0031 loss_train: 1.4844 acc_train: 0.3786 loss_val: 1.5348 acc_val: 0.3767\n",
      "Epoch: 0032 loss_train: 1.4600 acc_train: 0.3929 loss_val: 1.5178 acc_val: 0.3733\n",
      "Epoch: 0033 loss_train: 1.4198 acc_train: 0.4000 loss_val: 1.5011 acc_val: 0.3867\n",
      "Epoch: 0034 loss_train: 1.4482 acc_train: 0.4143 loss_val: 1.4847 acc_val: 0.3900\n",
      "Epoch: 0035 loss_train: 1.4159 acc_train: 0.4214 loss_val: 1.4687 acc_val: 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53/200 [00:00<00:01, 132.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0036 loss_train: 1.3852 acc_train: 0.4571 loss_val: 1.4531 acc_val: 0.4000\n",
      "Epoch: 0037 loss_train: 1.3653 acc_train: 0.4500 loss_val: 1.4379 acc_val: 0.4100\n",
      "Epoch: 0038 loss_train: 1.3160 acc_train: 0.4786 loss_val: 1.4230 acc_val: 0.4200\n",
      "Epoch: 0039 loss_train: 1.3203 acc_train: 0.4786 loss_val: 1.4085 acc_val: 0.4233\n",
      "Epoch: 0040 loss_train: 1.3027 acc_train: 0.4857 loss_val: 1.3945 acc_val: 0.4433\n",
      "Epoch: 0041 loss_train: 1.3113 acc_train: 0.5214 loss_val: 1.3807 acc_val: 0.4500\n",
      "Epoch: 0042 loss_train: 1.2674 acc_train: 0.4857 loss_val: 1.3675 acc_val: 0.4600\n",
      "Epoch: 0043 loss_train: 1.2290 acc_train: 0.5643 loss_val: 1.3546 acc_val: 0.4667\n",
      "Epoch: 0044 loss_train: 1.2270 acc_train: 0.5500 loss_val: 1.3420 acc_val: 0.4800\n",
      "Epoch: 0045 loss_train: 1.2216 acc_train: 0.5500 loss_val: 1.3298 acc_val: 0.4833\n",
      "Epoch: 0046 loss_train: 1.1960 acc_train: 0.5714 loss_val: 1.3179 acc_val: 0.5067\n",
      "Epoch: 0047 loss_train: 1.1859 acc_train: 0.5857 loss_val: 1.3064 acc_val: 0.5233\n",
      "Epoch: 0048 loss_train: 1.1679 acc_train: 0.6429 loss_val: 1.2951 acc_val: 0.5333\n",
      "Epoch: 0049 loss_train: 1.1431 acc_train: 0.6286 loss_val: 1.2841 acc_val: 0.5467\n",
      "Epoch: 0050 loss_train: 1.1530 acc_train: 0.6286 loss_val: 1.2734 acc_val: 0.5567\n",
      "Epoch: 0051 loss_train: 1.1182 acc_train: 0.6286 loss_val: 1.2629 acc_val: 0.5667\n",
      "Epoch: 0052 loss_train: 1.1386 acc_train: 0.6643 loss_val: 1.2526 acc_val: 0.5900\n",
      "Epoch: 0053 loss_train: 1.1200 acc_train: 0.6643 loss_val: 1.2425 acc_val: 0.5900\n",
      "Epoch: 0054 loss_train: 1.0596 acc_train: 0.7071 loss_val: 1.2325 acc_val: 0.6000\n",
      "Epoch: 0055 loss_train: 1.0863 acc_train: 0.6857 loss_val: 1.2228 acc_val: 0.6133\n",
      "Epoch: 0056 loss_train: 1.0647 acc_train: 0.6786 loss_val: 1.2132 acc_val: 0.6133\n",
      "Epoch: 0057 loss_train: 1.0298 acc_train: 0.7000 loss_val: 1.2037 acc_val: 0.6167\n",
      "Epoch: 0058 loss_train: 1.0439 acc_train: 0.6857 loss_val: 1.1943 acc_val: 0.6200\n",
      "Epoch: 0059 loss_train: 1.0444 acc_train: 0.7357 loss_val: 1.1850 acc_val: 0.6267\n",
      "Epoch: 0060 loss_train: 1.0126 acc_train: 0.7143 loss_val: 1.1759 acc_val: 0.6333\n",
      "Epoch: 0061 loss_train: 0.9799 acc_train: 0.6714 loss_val: 1.1669 acc_val: 0.6467\n",
      "Epoch: 0062 loss_train: 0.9715 acc_train: 0.7643 loss_val: 1.1580 acc_val: 0.6567\n",
      "Epoch: 0063 loss_train: 0.9713 acc_train: 0.7357 loss_val: 1.1493 acc_val: 0.6633\n",
      "Epoch: 0064 loss_train: 0.9631 acc_train: 0.7714 loss_val: 1.1407 acc_val: 0.6700\n",
      "Epoch: 0065 loss_train: 0.9552 acc_train: 0.7286 loss_val: 1.1323 acc_val: 0.6700\n",
      "Epoch: 0066 loss_train: 0.9474 acc_train: 0.7286 loss_val: 1.1241 acc_val: 0.6700\n",
      "Epoch: 0067 loss_train: 0.9180 acc_train: 0.8000 loss_val: 1.1160 acc_val: 0.6800\n",
      "Epoch: 0068 loss_train: 0.8944 acc_train: 0.7643 loss_val: 1.1080 acc_val: 0.6867\n",
      "Epoch: 0069 loss_train: 0.9045 acc_train: 0.7857 loss_val: 1.1001 acc_val: 0.7033\n",
      "Epoch: 0070 loss_train: 0.8934 acc_train: 0.8000 loss_val: 1.0921 acc_val: 0.7067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 89/200 [00:00<00:00, 154.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0071 loss_train: 0.8796 acc_train: 0.8286 loss_val: 1.0841 acc_val: 0.7133\n",
      "Epoch: 0072 loss_train: 0.8982 acc_train: 0.8000 loss_val: 1.0763 acc_val: 0.7167\n",
      "Epoch: 0073 loss_train: 0.8419 acc_train: 0.8071 loss_val: 1.0686 acc_val: 0.7233\n",
      "Epoch: 0074 loss_train: 0.8535 acc_train: 0.8214 loss_val: 1.0611 acc_val: 0.7233\n",
      "Epoch: 0075 loss_train: 0.8151 acc_train: 0.8500 loss_val: 1.0536 acc_val: 0.7200\n",
      "Epoch: 0076 loss_train: 0.8162 acc_train: 0.8429 loss_val: 1.0462 acc_val: 0.7333\n",
      "Epoch: 0077 loss_train: 0.8483 acc_train: 0.8143 loss_val: 1.0390 acc_val: 0.7333\n",
      "Epoch: 0078 loss_train: 0.8222 acc_train: 0.8500 loss_val: 1.0318 acc_val: 0.7400\n",
      "Epoch: 0079 loss_train: 0.8259 acc_train: 0.8286 loss_val: 1.0249 acc_val: 0.7467\n",
      "Epoch: 0080 loss_train: 0.8149 acc_train: 0.8500 loss_val: 1.0179 acc_val: 0.7500\n",
      "Epoch: 0081 loss_train: 0.7499 acc_train: 0.8643 loss_val: 1.0109 acc_val: 0.7500\n",
      "Epoch: 0082 loss_train: 0.7804 acc_train: 0.8500 loss_val: 1.0041 acc_val: 0.7533\n",
      "Epoch: 0083 loss_train: 0.7711 acc_train: 0.8714 loss_val: 0.9974 acc_val: 0.7567\n",
      "Epoch: 0084 loss_train: 0.7844 acc_train: 0.8500 loss_val: 0.9908 acc_val: 0.7600\n",
      "Epoch: 0085 loss_train: 0.7767 acc_train: 0.8857 loss_val: 0.9843 acc_val: 0.7600\n",
      "Epoch: 0086 loss_train: 0.7505 acc_train: 0.8571 loss_val: 0.9780 acc_val: 0.7633\n",
      "Epoch: 0087 loss_train: 0.7119 acc_train: 0.8714 loss_val: 0.9718 acc_val: 0.7667\n",
      "Epoch: 0088 loss_train: 0.7094 acc_train: 0.8714 loss_val: 0.9657 acc_val: 0.7667\n",
      "Epoch: 0089 loss_train: 0.7533 acc_train: 0.8714 loss_val: 0.9597 acc_val: 0.7700\n",
      "Epoch: 0090 loss_train: 0.7070 acc_train: 0.8500 loss_val: 0.9539 acc_val: 0.7767\n",
      "Epoch: 0091 loss_train: 0.6989 acc_train: 0.8643 loss_val: 0.9485 acc_val: 0.7767\n",
      "Epoch: 0092 loss_train: 0.7148 acc_train: 0.8786 loss_val: 0.9432 acc_val: 0.7767\n",
      "Epoch: 0093 loss_train: 0.7016 acc_train: 0.8857 loss_val: 0.9380 acc_val: 0.7767\n",
      "Epoch: 0094 loss_train: 0.6921 acc_train: 0.8714 loss_val: 0.9329 acc_val: 0.7767\n",
      "Epoch: 0095 loss_train: 0.6968 acc_train: 0.8500 loss_val: 0.9280 acc_val: 0.7767\n",
      "Epoch: 0096 loss_train: 0.6713 acc_train: 0.8714 loss_val: 0.9233 acc_val: 0.7800\n",
      "Epoch: 0097 loss_train: 0.6667 acc_train: 0.8500 loss_val: 0.9190 acc_val: 0.7833\n",
      "Epoch: 0098 loss_train: 0.6494 acc_train: 0.8786 loss_val: 0.9148 acc_val: 0.7900\n",
      "Epoch: 0099 loss_train: 0.6737 acc_train: 0.8786 loss_val: 0.9104 acc_val: 0.7933\n",
      "Epoch: 0100 loss_train: 0.6630 acc_train: 0.9000 loss_val: 0.9062 acc_val: 0.7933\n",
      "Epoch: 0101 loss_train: 0.6696 acc_train: 0.8500 loss_val: 0.9021 acc_val: 0.7933\n",
      "Epoch: 0102 loss_train: 0.6519 acc_train: 0.8857 loss_val: 0.8981 acc_val: 0.7967\n",
      "Epoch: 0103 loss_train: 0.6449 acc_train: 0.9143 loss_val: 0.8941 acc_val: 0.8000\n",
      "Epoch: 0104 loss_train: 0.6240 acc_train: 0.8857 loss_val: 0.8903 acc_val: 0.8000\n",
      "Epoch: 0105 loss_train: 0.6492 acc_train: 0.8929 loss_val: 0.8864 acc_val: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 125/200 [00:00<00:00, 164.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0106 loss_train: 0.6143 acc_train: 0.8929 loss_val: 0.8826 acc_val: 0.8000\n",
      "Epoch: 0107 loss_train: 0.6353 acc_train: 0.8571 loss_val: 0.8790 acc_val: 0.8000\n",
      "Epoch: 0108 loss_train: 0.6018 acc_train: 0.8643 loss_val: 0.8754 acc_val: 0.8000\n",
      "Epoch: 0109 loss_train: 0.6147 acc_train: 0.8857 loss_val: 0.8719 acc_val: 0.8000\n",
      "Epoch: 0110 loss_train: 0.5641 acc_train: 0.9214 loss_val: 0.8684 acc_val: 0.8000\n",
      "Epoch: 0111 loss_train: 0.6000 acc_train: 0.9000 loss_val: 0.8652 acc_val: 0.8000\n",
      "Epoch: 0112 loss_train: 0.6180 acc_train: 0.8929 loss_val: 0.8621 acc_val: 0.8000\n",
      "Epoch: 0113 loss_train: 0.5977 acc_train: 0.9000 loss_val: 0.8587 acc_val: 0.8033\n",
      "Epoch: 0114 loss_train: 0.5994 acc_train: 0.9143 loss_val: 0.8550 acc_val: 0.8033\n",
      "Epoch: 0115 loss_train: 0.5606 acc_train: 0.9286 loss_val: 0.8514 acc_val: 0.8067\n",
      "Epoch: 0116 loss_train: 0.5843 acc_train: 0.9000 loss_val: 0.8480 acc_val: 0.8067\n",
      "Epoch: 0117 loss_train: 0.5674 acc_train: 0.9000 loss_val: 0.8446 acc_val: 0.8067\n",
      "Epoch: 0118 loss_train: 0.5693 acc_train: 0.9000 loss_val: 0.8413 acc_val: 0.8067\n",
      "Epoch: 0119 loss_train: 0.5688 acc_train: 0.8857 loss_val: 0.8382 acc_val: 0.8067\n",
      "Epoch: 0120 loss_train: 0.5382 acc_train: 0.9071 loss_val: 0.8351 acc_val: 0.8067\n",
      "Epoch: 0121 loss_train: 0.5612 acc_train: 0.9000 loss_val: 0.8320 acc_val: 0.8067\n",
      "Epoch: 0122 loss_train: 0.5509 acc_train: 0.9143 loss_val: 0.8288 acc_val: 0.8100\n",
      "Epoch: 0123 loss_train: 0.5403 acc_train: 0.9214 loss_val: 0.8260 acc_val: 0.8100\n",
      "Epoch: 0124 loss_train: 0.5382 acc_train: 0.9071 loss_val: 0.8233 acc_val: 0.8100\n",
      "Epoch: 0125 loss_train: 0.5207 acc_train: 0.9143 loss_val: 0.8209 acc_val: 0.8100\n",
      "Epoch: 0126 loss_train: 0.5263 acc_train: 0.9071 loss_val: 0.8185 acc_val: 0.8100\n",
      "Epoch: 0127 loss_train: 0.5518 acc_train: 0.8929 loss_val: 0.8161 acc_val: 0.8100\n",
      "Epoch: 0128 loss_train: 0.5259 acc_train: 0.9071 loss_val: 0.8136 acc_val: 0.8100\n",
      "Epoch: 0129 loss_train: 0.5125 acc_train: 0.9214 loss_val: 0.8114 acc_val: 0.8100\n",
      "Epoch: 0130 loss_train: 0.5138 acc_train: 0.9143 loss_val: 0.8095 acc_val: 0.8133\n",
      "Epoch: 0131 loss_train: 0.5529 acc_train: 0.9071 loss_val: 0.8076 acc_val: 0.8100\n",
      "Epoch: 0132 loss_train: 0.5111 acc_train: 0.9071 loss_val: 0.8054 acc_val: 0.8100\n",
      "Epoch: 0133 loss_train: 0.5111 acc_train: 0.9214 loss_val: 0.8031 acc_val: 0.8100\n",
      "Epoch: 0134 loss_train: 0.5190 acc_train: 0.9143 loss_val: 0.8012 acc_val: 0.8100\n",
      "Epoch: 0135 loss_train: 0.5252 acc_train: 0.9214 loss_val: 0.7994 acc_val: 0.8100\n",
      "Epoch: 0136 loss_train: 0.5349 acc_train: 0.9071 loss_val: 0.7975 acc_val: 0.8133\n",
      "Epoch: 0137 loss_train: 0.5085 acc_train: 0.9143 loss_val: 0.7955 acc_val: 0.8167\n",
      "Epoch: 0138 loss_train: 0.4653 acc_train: 0.9500 loss_val: 0.7935 acc_val: 0.8167\n",
      "Epoch: 0139 loss_train: 0.4808 acc_train: 0.9500 loss_val: 0.7915 acc_val: 0.8167\n",
      "Epoch: 0140 loss_train: 0.4751 acc_train: 0.9143 loss_val: 0.7894 acc_val: 0.8167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [00:01<00:00, 169.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0141 loss_train: 0.4805 acc_train: 0.9214 loss_val: 0.7875 acc_val: 0.8167\n",
      "Epoch: 0142 loss_train: 0.5226 acc_train: 0.9214 loss_val: 0.7857 acc_val: 0.8133\n",
      "Epoch: 0143 loss_train: 0.4708 acc_train: 0.9214 loss_val: 0.7837 acc_val: 0.8133\n",
      "Epoch: 0144 loss_train: 0.4417 acc_train: 0.9357 loss_val: 0.7820 acc_val: 0.8133\n",
      "Epoch: 0145 loss_train: 0.4707 acc_train: 0.9357 loss_val: 0.7803 acc_val: 0.8100\n",
      "Epoch: 0146 loss_train: 0.4779 acc_train: 0.9143 loss_val: 0.7786 acc_val: 0.8100\n",
      "Epoch: 0147 loss_train: 0.4796 acc_train: 0.9143 loss_val: 0.7768 acc_val: 0.8100\n",
      "Epoch: 0148 loss_train: 0.4731 acc_train: 0.9143 loss_val: 0.7748 acc_val: 0.8100\n",
      "Epoch: 0149 loss_train: 0.5068 acc_train: 0.9143 loss_val: 0.7731 acc_val: 0.8100\n",
      "Epoch: 0150 loss_train: 0.4445 acc_train: 0.9286 loss_val: 0.7714 acc_val: 0.8100\n",
      "Epoch: 0151 loss_train: 0.4687 acc_train: 0.9429 loss_val: 0.7697 acc_val: 0.8100\n",
      "Epoch: 0152 loss_train: 0.4601 acc_train: 0.9429 loss_val: 0.7679 acc_val: 0.8133\n",
      "Epoch: 0153 loss_train: 0.4864 acc_train: 0.9214 loss_val: 0.7661 acc_val: 0.8133\n",
      "Epoch: 0154 loss_train: 0.4627 acc_train: 0.9143 loss_val: 0.7641 acc_val: 0.8133\n",
      "Epoch: 0155 loss_train: 0.4854 acc_train: 0.9214 loss_val: 0.7623 acc_val: 0.8167\n",
      "Epoch: 0156 loss_train: 0.4647 acc_train: 0.9357 loss_val: 0.7608 acc_val: 0.8167\n",
      "Epoch: 0157 loss_train: 0.4810 acc_train: 0.9143 loss_val: 0.7594 acc_val: 0.8200\n",
      "Epoch: 0158 loss_train: 0.4956 acc_train: 0.9000 loss_val: 0.7582 acc_val: 0.8200\n",
      "Epoch: 0159 loss_train: 0.4464 acc_train: 0.9500 loss_val: 0.7572 acc_val: 0.8167\n",
      "Epoch: 0160 loss_train: 0.4610 acc_train: 0.9429 loss_val: 0.7559 acc_val: 0.8100\n",
      "Epoch: 0161 loss_train: 0.4634 acc_train: 0.9071 loss_val: 0.7547 acc_val: 0.8100\n",
      "Epoch: 0162 loss_train: 0.4318 acc_train: 0.9429 loss_val: 0.7535 acc_val: 0.8067\n",
      "Epoch: 0163 loss_train: 0.4508 acc_train: 0.9286 loss_val: 0.7525 acc_val: 0.8033\n",
      "Epoch: 0164 loss_train: 0.4312 acc_train: 0.9500 loss_val: 0.7515 acc_val: 0.8033\n",
      "Epoch: 0165 loss_train: 0.4834 acc_train: 0.9214 loss_val: 0.7505 acc_val: 0.8033\n",
      "Epoch: 0166 loss_train: 0.4431 acc_train: 0.9286 loss_val: 0.7493 acc_val: 0.8033\n",
      "Epoch: 0167 loss_train: 0.4693 acc_train: 0.9357 loss_val: 0.7480 acc_val: 0.8067\n",
      "Epoch: 0168 loss_train: 0.4495 acc_train: 0.9500 loss_val: 0.7465 acc_val: 0.8067\n",
      "Epoch: 0169 loss_train: 0.4240 acc_train: 0.9357 loss_val: 0.7453 acc_val: 0.8067\n",
      "Epoch: 0170 loss_train: 0.4479 acc_train: 0.9071 loss_val: 0.7441 acc_val: 0.8067\n",
      "Epoch: 0171 loss_train: 0.4304 acc_train: 0.9571 loss_val: 0.7428 acc_val: 0.8067\n",
      "Epoch: 0172 loss_train: 0.4394 acc_train: 0.9357 loss_val: 0.7415 acc_val: 0.8067\n",
      "Epoch: 0173 loss_train: 0.4316 acc_train: 0.9214 loss_val: 0.7405 acc_val: 0.8100\n",
      "Epoch: 0174 loss_train: 0.4249 acc_train: 0.9214 loss_val: 0.7398 acc_val: 0.8067\n",
      "Epoch: 0175 loss_train: 0.4531 acc_train: 0.9143 loss_val: 0.7394 acc_val: 0.8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 148.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0176 loss_train: 0.4161 acc_train: 0.9429 loss_val: 0.7390 acc_val: 0.8033\n",
      "Epoch: 0177 loss_train: 0.4252 acc_train: 0.9643 loss_val: 0.7386 acc_val: 0.8033\n",
      "Epoch: 0178 loss_train: 0.4175 acc_train: 0.9571 loss_val: 0.7378 acc_val: 0.8033\n",
      "Epoch: 0179 loss_train: 0.4310 acc_train: 0.9286 loss_val: 0.7366 acc_val: 0.8033\n",
      "Epoch: 0180 loss_train: 0.4287 acc_train: 0.9357 loss_val: 0.7352 acc_val: 0.8033\n",
      "Epoch: 0181 loss_train: 0.4172 acc_train: 0.9643 loss_val: 0.7337 acc_val: 0.8033\n",
      "Epoch: 0182 loss_train: 0.4095 acc_train: 0.9500 loss_val: 0.7325 acc_val: 0.8067\n",
      "Epoch: 0183 loss_train: 0.4446 acc_train: 0.9429 loss_val: 0.7313 acc_val: 0.8067\n",
      "Epoch: 0184 loss_train: 0.4025 acc_train: 0.9357 loss_val: 0.7298 acc_val: 0.8067\n",
      "Epoch: 0185 loss_train: 0.4447 acc_train: 0.9143 loss_val: 0.7283 acc_val: 0.8067\n",
      "Epoch: 0186 loss_train: 0.3976 acc_train: 0.9571 loss_val: 0.7266 acc_val: 0.8100\n",
      "Epoch: 0187 loss_train: 0.4490 acc_train: 0.9143 loss_val: 0.7252 acc_val: 0.8100\n",
      "Epoch: 0188 loss_train: 0.4326 acc_train: 0.9357 loss_val: 0.7241 acc_val: 0.8100\n",
      "Epoch: 0189 loss_train: 0.4261 acc_train: 0.9286 loss_val: 0.7230 acc_val: 0.8100\n",
      "Epoch: 0190 loss_train: 0.3675 acc_train: 0.9429 loss_val: 0.7224 acc_val: 0.8100\n",
      "Epoch: 0191 loss_train: 0.3917 acc_train: 0.9500 loss_val: 0.7218 acc_val: 0.8100\n",
      "Epoch: 0192 loss_train: 0.3785 acc_train: 0.9357 loss_val: 0.7212 acc_val: 0.8133\n",
      "Epoch: 0193 loss_train: 0.4246 acc_train: 0.9357 loss_val: 0.7204 acc_val: 0.8133\n",
      "Epoch: 0194 loss_train: 0.3859 acc_train: 0.9643 loss_val: 0.7199 acc_val: 0.8167\n",
      "Epoch: 0195 loss_train: 0.3578 acc_train: 0.9571 loss_val: 0.7191 acc_val: 0.8167\n",
      "Epoch: 0196 loss_train: 0.4204 acc_train: 0.9357 loss_val: 0.7184 acc_val: 0.8167\n",
      "Epoch: 0197 loss_train: 0.3915 acc_train: 0.9429 loss_val: 0.7176 acc_val: 0.8200\n",
      "Epoch: 0198 loss_train: 0.4030 acc_train: 0.9357 loss_val: 0.7173 acc_val: 0.8200\n",
      "Epoch: 0199 loss_train: 0.3617 acc_train: 0.9643 loss_val: 0.7168 acc_val: 0.8167\n",
      "Epoch: 0200 loss_train: 0.3799 acc_train: 0.9571 loss_val: 0.7165 acc_val: 0.8167\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
